{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjhazcG6ttnQGAqPhyvwsh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dankodanny/fromthescratch/blob/master/10models_JACS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install rdkit\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor, ExtraTreesRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import cross_val_score  # Add this import\n",
        "\n",
        "import optuna\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit import DataStructs\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import SimilarityMaps\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import MACCSkeys\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_smiles_encodings_for_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    smiles_list = np.asanyarray(df.SMILES)\n",
        "    s1_energy_list = np.asarray(df['Redox'])\n",
        "    smiles_alphabet = list(set(''.join(smiles_list)))\n",
        "    smiles_alphabet.append(' ')  # for padding\n",
        "    largest_smiles_len = len(max(smiles_list, key=len))\n",
        "    # print('Finished translating SMILES to SELFIES.')\n",
        "    return smiles_list, smiles_alphabet, largest_smiles_len, s1_energy_list\n",
        "\n",
        "def smile_to_hot(smile, alphabet, largest_smile_len):\n",
        "    \"\"\"Go from a single smile string to a one-hot encoding.\n",
        "    \"\"\"\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "    # pad with ' '\n",
        "    smile += ' ' * (largest_smile_len - len(smile))\n",
        "    # integer encode input smile\n",
        "    integer_encoded = [char_to_int[c] for char in smile for c in char]\n",
        "    # one hot-encode input smile\n",
        "    onehot_encoded = deque()\n",
        "    for value in integer_encoded:\n",
        "        letter = [0 for _ in range(len(alphabet))]\n",
        "        letter[value] = 1\n",
        "        onehot_encoded.append(letter)\n",
        "    onehot_result = deque()\n",
        "    # 265*40 -> 10600으로 변환\n",
        "    for row in onehot_encoded:\n",
        "        onehot_result.extend(row)\n",
        "    return integer_encoded, np.array(list(onehot_result))\n",
        "\n",
        "def multiple_smile_to_hot(smiles_list, largest_molecule_len, alphabet):\n",
        "    \"\"\"Convert a list of smile strings to a one-hot encoding\n",
        "    Returned shape (num_smiles x len_of_largest_smile x len_smile_encoding)\n",
        "    \"\"\"\n",
        "    hot_list = []\n",
        "    for s in tqdm(smiles_list, desc=\"S2H\"):\n",
        "        _, onehot_encoded = smile_to_hot(s, largest_molecule_len, alphabet)\n",
        "        hot_list.append(onehot_encoded)\n",
        "    # hot_list = numpy_reshape(hot_list)\n",
        "    return np.array(hot_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke__2qcUFJg_",
        "outputId": "f4462ed3-7a77-48c8-a60a-b4f21b1c85f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.24)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Smiles 2 hot input\n",
        "'''\n",
        "\n",
        "# Create synthetic data for testing\n",
        "\n",
        "path = './train.csv'\n",
        "smiles_list, alphabet, largest_smile_len, train_y = get_smiles_encodings_for_dataset(path)\n",
        "hot_list = multiple_smile_to_hot(smiles_list, alphabet, largest_smile_len)      # hot_list는 np.array\n",
        "\n",
        "# X, Y setting\n",
        "X, y = hot_list, np.array(train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "44JDLUKKIEuw",
        "outputId": "44da5b78-5630-4587-bab8-579dd84f0b21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1edc561fd482>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msmiles_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlargest_smile_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_smiles_encodings_for_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mhot_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiple_smile_to_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlargest_smile_len\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# hot_list는 np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-16291d77d4c0>\u001b[0m in \u001b[0;36mget_smiles_encodings_for_dataset\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_smiles_encodings_for_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0msmiles_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSMILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0ms1_energy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Redox'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Finger print input\n",
        "'''\n",
        "\n",
        "ffpp = \"pattern\"\n",
        "\n",
        "train = pd.read_csv(\"./train.csv\")\n",
        "\n",
        "train_fps = []#train fingerprints\n",
        "train_y = [] #train y(label)\n",
        "\n",
        "for index, row in train.iterrows() :\n",
        "  try :\n",
        "    mol = Chem.MolFromSmiles(row['SMILES'])\n",
        "    if ffpp == 'maccs' :\n",
        "        fp = MACCSkeys.GenMACCSKeys(mol)\n",
        "    elif ffpp == 'morgan' :\n",
        "        fp = Chem.AllChem.GetMorganFingerprintAsBitVect(mol, 4)\n",
        "    elif ffpp == 'rdkit' :\n",
        "        fp = Chem.RDKFingerprint(mol)\n",
        "    elif ffpp == 'pattern' :\n",
        "        fp = Chem.rdmolops.PatternFingerprint(mol)\n",
        "    elif ffpp == 'layerd' :\n",
        "        fp = Chem.rdmolops.LayeredFingerprint(mol)\n",
        "\n",
        "    train_fps.append(fp)\n",
        "    train_y.append(row['Redox'])\n",
        "  except :\n",
        "    pass\n",
        "\n",
        "#fingerfrint object to ndarray\n",
        "np_train_fps = []\n",
        "for fp in train_fps:\n",
        "  arr = np.zeros((0,))\n",
        "  DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "  np_train_fps.append(arr)\n",
        "\n",
        "np_train_fps_array = np.array(np_train_fps)\n",
        "\n",
        "X, y = np_train_fps_array , np.array(train_y)\n"
      ],
      "metadata": {
        "id": "G5FSESnRh4Pv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models and their corresponding objective functions\n",
        "models = {\n",
        "    \"LinearRegression\": (LinearRegression, {\"fit_intercept\": None, \"positive\": None}),\n",
        "    \"Ridge\": (Ridge, {\"alpha\": None}),\n",
        "    \"DecisionTreeRegressor\": (DecisionTreeRegressor, {\"criterion\": None, \"max_depth\": None, \"min_samples_leaf\": None}),\n",
        "    \"SVR\": (SVR, {\"kernel\": None, \"degree\": None}),\n",
        "    \"KNeighborsRegressor\": (KNeighborsRegressor, {\"n_neighbors\": None, \"weights\": None}),\n",
        "    \"RandomForestRegressor\": (RandomForestRegressor, {\"n_estimators\": None, \"criterion\": None, \"max_depth\": None, \"min_samples_leaf\": None}),\n",
        "    \"AdaBoostRegressor\": (AdaBoostRegressor, {\"n_estimators\": None, \"learning_rate\": None, \"loss\": None}),\n",
        "    \"GradientBoostingRegressor\": (GradientBoostingRegressor, {\"n_estimators\": None, \"learning_rate\": None, \"loss\": None}),\n",
        "    \"BaggingRegressor\": (BaggingRegressor, {\"n_estimators\": None, \"max_samples\": None, \"max_features\": None}),\n",
        "    \"ExtraTreesRegressor\": (ExtraTreesRegressor, {\"criterion\": None, \"max_depth\": None, \"min_samples_leaf\": None}),\n",
        "    \"MLPRegressor\": (MLPRegressor, {\"n_layers\": None, \"layers\": None, \"lr\": None, \"alpha\": None}),\n",
        "    \"LGBMRegressor\": (LGBMRegressor, {\"num_leaves\": None, \"min_data_in_leaf\": None, \"max_depth\": None, \"learning_rate\": None, \"max_bin\": None}),\n",
        "}\n",
        "\n",
        "def objective(trial, model_name):\n",
        "    model_class, param_grid = models[model_name]\n",
        "\n",
        "    model_params = {}\n",
        "    if model_name == \"MLPRegressor\":\n",
        "        n_layers = trial.suggest_int('n_layers', 1, 5)\n",
        "        layers = [int(trial.suggest_loguniform('n_units_l{}'.format(i), 4, 200)) for i in range(n_layers)]\n",
        "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
        "        alpha = trial.suggest_loguniform('alpha', 1e-6, 0.1)\n",
        "        model_params[\"n_layers\"] = n_layers\n",
        "        model_params[\"layers\"] = layers\n",
        "        model_params[\"lr\"] = lr\n",
        "        model_params[\"alpha\"] = alpha\n",
        "\n",
        "    elif model_name == \"LGBMRegressor\":\n",
        "        num_leaves = trial.suggest_int('num_leaves', 5, 100)\n",
        "        min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 10, 100)\n",
        "        max_depth = trial.suggest_int('max_depth', -1, 10)\n",
        "        lr = trial.suggest_loguniform('lr', 1e-5, 1.0)\n",
        "        max_bin = trial.suggest_int('max_bin', 10, 500)\n",
        "        model_params[\"num_leaves\"] = num_leaves\n",
        "        model_params[\"min_data_in_leaf\"] = min_data_in_leaf\n",
        "        model_params[\"max_depth\"] = max_depth\n",
        "        model_params[\"lr\"] = lr\n",
        "        model_params[\"max_bin\"] = max_bin\n",
        "\n",
        "    else:\n",
        "        # Add your code block here for other models\n",
        "        if model_name == \"LinearRegression\":\n",
        "            fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
        "            positive = trial.suggest_categorical(\"positive\", [True, False])\n",
        "            model_params[\"fit_intercept\"] = fit_intercept\n",
        "            model_params[\"positive\"] = positive\n",
        "\n",
        "        elif model_name == \"DecisionTreeRegressor\":\n",
        "            criterion = trial.suggest_categorical(\"criterion\", [\"squared_error\", \"friedman_mse\", \"absolute_error\"])\n",
        "            max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n",
        "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "            model_params[\"criterion\"] = criterion\n",
        "            model_params[\"max_depth\"] = max_depth\n",
        "            model_params[\"min_samples_leaf\"] = min_samples_leaf\n",
        "\n",
        "        elif model_name == \"SVR\":\n",
        "            kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\"])\n",
        "            degree = trial.suggest_int(\"degree\", 2, 5)\n",
        "            model_params[\"kernel\"] = kernel\n",
        "            model_params[\"degree\"] = degree\n",
        "\n",
        "        elif model_name == \"KNeighborsRegressor\":\n",
        "            n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 10)\n",
        "            weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
        "            model_params[\"n_neighbors\"] = n_neighbors\n",
        "            model_params[\"weights\"] = weights\n",
        "\n",
        "        elif model_name == \"RandomForestRegressor\":\n",
        "            n_estimators = trial.suggest_int(\"n_estimators\", 10, 200)\n",
        "            criterion = trial.suggest_categorical(\"criterion\", [\"squared_error\", \"friedman_mse\", \"absolute_error\"])\n",
        "            max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n",
        "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "            model_params[\"n_estimators\"] = n_estimators\n",
        "            model_params[\"criterion\"] = criterion\n",
        "            model_params[\"max_depth\"] = max_depth\n",
        "            model_params[\"min_samples_leaf\"] = min_samples_leaf\n",
        "\n",
        "        elif model_name == \"AdaBoostRegressor\":\n",
        "            n_estimators = trial.suggest_int(\"n_estimators\", 10, 200)\n",
        "            learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
        "            loss = trial.suggest_categorical(\"loss\", [\"linear\", \"square\", \"exponential\"])\n",
        "            model_params[\"n_estimators\"] = n_estimators\n",
        "            model_params[\"learning_rate\"] = learning_rate\n",
        "            model_params[\"loss\"] = loss\n",
        "\n",
        "        elif model_name == \"GradientBoostingRegressor\":\n",
        "            n_estimators = trial.suggest_int(\"n_estimators\", 10, 200)\n",
        "            learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
        "            loss = trial.suggest_categorical(\"loss\", ['squared_error', 'quantile', 'huber', 'absolute_error'])\n",
        "            model_params[\"n_estimators\"] = n_estimators\n",
        "            model_params[\"learning_rate\"] = learning_rate\n",
        "            model_params[\"loss\"] = loss\n",
        "\n",
        "        elif model_name == \"BaggingRegressor\":\n",
        "            n_estimators = trial.suggest_int(\"n_estimators\", 10, 200)\n",
        "            max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
        "            max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
        "            model_params[\"n_estimators\"] = n_estimators\n",
        "            model_params[\"max_samples\"] = max_samples\n",
        "            model_params[\"max_features\"] = max_features\n",
        "\n",
        "        elif model_name == \"ExtraTreesRegressor\":\n",
        "            criterion = trial.suggest_categorical(\"criterion\", [ 'friedman_mse', 'squared_error', 'absolute_error'])\n",
        "            max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n",
        "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "            model_params[\"criterion\"] = criterion\n",
        "            model_params[\"max_depth\"] = max_depth\n",
        "            model_params[\"min_samples_leaf\"] = min_samples_leaf\n",
        "\n",
        "        elif model_name == \"Ridge\":\n",
        "            alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
        "            model_params[\"alpha\"] = alpha\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid model_name: {model_name}\")\n",
        "\n",
        "    # Create the model with optimized hyperparameters\n",
        "    model = model_class(**model_params)\n",
        "\n",
        "    # Perform cross-validation and return negative mean absolute error (MAE)\n",
        "    mae_values = -cross_val_score(model, X, y, cv=4, scoring=\"neg_mean_absolute_error\")\n",
        "    return np.mean(mae_values)\n",
        "\n",
        "# Create an Optuna study for each model\n",
        "study_results = {}\n",
        "for model_name in models.keys():\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(lambda trial: objective(trial, model_name), n_trials=10)\n",
        "    study_results[model_name] = study\n",
        "\n",
        "# Print the optimized hyperparameters for each model\n",
        "for model_name, study in study_results.items():\n",
        "    print(f\"{model_name}에 대한 최적 하이퍼파라미터:\")\n",
        "    for key, value in study.best_params.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLvhv8VdXT_S",
        "outputId": "b961ac9f-ee5f-4caa-a601-e3c0b84821ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-11 02:26:31,667] A new study created in memory with name: no-name-0ff55e33-bcb1-408e-923d-97eccf25a4a7\n",
            "[I 2024-01-11 02:26:48,384] Trial 0 finished with value: 0.7976786938254727 and parameters: {'fit_intercept': True, 'positive': True}. Best is trial 0 with value: 0.7976786938254727.\n",
            "[I 2024-01-11 02:27:02,456] Trial 1 finished with value: 0.785315528609059 and parameters: {'fit_intercept': False, 'positive': True}. Best is trial 1 with value: 0.785315528609059.\n",
            "[I 2024-01-11 02:27:17,725] Trial 2 finished with value: 0.7976786938254727 and parameters: {'fit_intercept': True, 'positive': True}. Best is trial 1 with value: 0.785315528609059.\n",
            "[I 2024-01-11 02:27:32,986] Trial 3 finished with value: 0.7976786938254727 and parameters: {'fit_intercept': True, 'positive': True}. Best is trial 1 with value: 0.785315528609059.\n",
            "[I 2024-01-11 02:27:44,734] Trial 4 finished with value: 344454419.15836567 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 1 with value: 0.785315528609059.\n",
            "[I 2024-01-11 02:27:57,580] Trial 5 finished with value: 13.83953309237363 and parameters: {'fit_intercept': False, 'positive': False}. Best is trial 1 with value: 0.785315528609059.\n",
            "[I 2024-01-11 02:28:09,444] Trial 6 finished with value: 13.83953309237363 and parameters: {'fit_intercept': False, 'positive': False}. Best is trial 1 with value: 0.785315528609059.\n",
            "[I 2024-01-11 02:28:19,130] Trial 7 finished with value: 344454419.15836567 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 1 with value: 0.785315528609059.\n",
            "[I 2024-01-11 02:28:30,514] Trial 8 finished with value: 344454419.15836567 and parameters: {'fit_intercept': True, 'positive': False}. Best is trial 1 with value: 0.785315528609059.\n",
            "[I 2024-01-11 02:28:41,906] Trial 9 finished with value: 13.83953309237363 and parameters: {'fit_intercept': False, 'positive': False}. Best is trial 1 with value: 0.785315528609059.\n",
            "[I 2024-01-11 02:28:41,917] A new study created in memory with name: no-name-8f3bc60d-0988-4d72-8f8b-d4882b90b38f\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:28:43,555] Trial 0 finished with value: 3.646302852027712 and parameters: {'alpha': 0.0001278798120292395}. Best is trial 0 with value: 3.646302852027712.\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:28:45,800] Trial 1 finished with value: 0.74439607367792 and parameters: {'alpha': 7.571201804485949}. Best is trial 1 with value: 0.74439607367792.\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:28:48,269] Trial 2 finished with value: 2.218364587936799 and parameters: {'alpha': 0.0010090847700697327}. Best is trial 1 with value: 0.74439607367792.\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:28:50,817] Trial 3 finished with value: 0.7501673465753719 and parameters: {'alpha': 2.819902464128484}. Best is trial 1 with value: 0.74439607367792.\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:28:52,740] Trial 4 finished with value: 3.368447631586518 and parameters: {'alpha': 0.00017393423738300384}. Best is trial 1 with value: 0.74439607367792.\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:28:54,286] Trial 5 finished with value: 0.7494786441199957 and parameters: {'alpha': 2.9843288588776433}. Best is trial 1 with value: 0.74439607367792.\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:28:55,823] Trial 6 finished with value: 1.260933478853003 and parameters: {'alpha': 0.012774587456950683}. Best is trial 1 with value: 0.74439607367792.\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:28:57,354] Trial 7 finished with value: 1.8066138712299298 and parameters: {'alpha': 0.0025235086858236017}. Best is trial 1 with value: 0.74439607367792.\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:28:58,881] Trial 8 finished with value: 0.7725026146684266 and parameters: {'alpha': 0.9633295544547794}. Best is trial 1 with value: 0.74439607367792.\n",
            "<ipython-input-11-3676543eb2d9>:114: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  alpha = trial.suggest_loguniform('alpha', 1e-4, 10)\n",
            "[I 2024-01-11 02:29:00,448] Trial 9 finished with value: 1.133487093340376 and parameters: {'alpha': 0.020718466751737497}. Best is trial 1 with value: 0.74439607367792.\n",
            "[I 2024-01-11 02:29:00,455] A new study created in memory with name: no-name-6c4035d6-b148-4215-a6d3-95cf0f3a0cd2\n",
            "[I 2024-01-11 02:29:00,988] Trial 0 finished with value: 0.8940182422051568 and parameters: {'criterion': 'squared_error', 'max_depth': 4, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8940182422051568.\n",
            "[I 2024-01-11 02:29:17,953] Trial 1 finished with value: 0.9048339807025028 and parameters: {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.8940182422051568.\n",
            "[I 2024-01-11 02:29:34,666] Trial 2 finished with value: 0.893185943042194 and parameters: {'criterion': 'absolute_error', 'max_depth': 9, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.893185943042194.\n",
            "[I 2024-01-11 02:29:39,544] Trial 3 finished with value: 0.9550805457561433 and parameters: {'criterion': 'absolute_error', 'max_depth': 1, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.893185943042194.\n",
            "[I 2024-01-11 02:29:39,915] Trial 4 finished with value: 0.8996407383494645 and parameters: {'criterion': 'squared_error', 'max_depth': 3, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.893185943042194.\n",
            "[I 2024-01-11 02:29:40,089] Trial 5 finished with value: 0.934886265756415 and parameters: {'criterion': 'friedman_mse', 'max_depth': 1, 'min_samples_leaf': 7}. Best is trial 2 with value: 0.893185943042194.\n",
            "[I 2024-01-11 02:29:40,865] Trial 6 finished with value: 0.9203507592605099 and parameters: {'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.893185943042194.\n",
            "[I 2024-01-11 02:29:56,314] Trial 7 finished with value: 0.9132250866359932 and parameters: {'criterion': 'absolute_error', 'max_depth': 4, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.893185943042194.\n",
            "[I 2024-01-11 02:29:57,233] Trial 8 finished with value: 0.8935012618292064 and parameters: {'criterion': 'friedman_mse', 'max_depth': 7, 'min_samples_leaf': 10}. Best is trial 2 with value: 0.893185943042194.\n",
            "[I 2024-01-11 02:29:57,507] Trial 9 finished with value: 0.934886265756415 and parameters: {'criterion': 'squared_error', 'max_depth': 1, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.893185943042194.\n",
            "[I 2024-01-11 02:29:57,514] A new study created in memory with name: no-name-1eeaa8ee-1fb6-459a-8db0-e8b5b25022c5\n",
            "[I 2024-01-11 02:30:59,574] Trial 0 finished with value: 0.8013770107656248 and parameters: {'kernel': 'linear', 'degree': 4}. Best is trial 0 with value: 0.8013770107656248.\n",
            "[I 2024-01-11 02:31:13,808] Trial 1 finished with value: 0.781373342091525 and parameters: {'kernel': 'poly', 'degree': 5}. Best is trial 1 with value: 0.781373342091525.\n",
            "[I 2024-01-11 02:31:29,948] Trial 2 finished with value: 0.7547473109911188 and parameters: {'kernel': 'rbf', 'degree': 4}. Best is trial 2 with value: 0.7547473109911188.\n",
            "[I 2024-01-11 02:32:33,257] Trial 3 finished with value: 0.8013770107656248 and parameters: {'kernel': 'linear', 'degree': 2}. Best is trial 2 with value: 0.7547473109911188.\n",
            "[I 2024-01-11 02:32:49,490] Trial 4 finished with value: 0.7547473109911188 and parameters: {'kernel': 'rbf', 'degree': 4}. Best is trial 2 with value: 0.7547473109911188.\n",
            "[I 2024-01-11 02:33:06,796] Trial 5 finished with value: 0.7547473109911188 and parameters: {'kernel': 'rbf', 'degree': 2}. Best is trial 2 with value: 0.7547473109911188.\n",
            "[I 2024-01-11 02:34:08,153] Trial 6 finished with value: 0.8013770107656248 and parameters: {'kernel': 'linear', 'degree': 2}. Best is trial 2 with value: 0.7547473109911188.\n",
            "[I 2024-01-11 02:34:21,673] Trial 7 finished with value: 0.7460811180026582 and parameters: {'kernel': 'poly', 'degree': 2}. Best is trial 7 with value: 0.7460811180026582.\n",
            "[I 2024-01-11 02:34:35,498] Trial 8 finished with value: 0.7567533432772666 and parameters: {'kernel': 'poly', 'degree': 3}. Best is trial 7 with value: 0.7460811180026582.\n",
            "[I 2024-01-11 02:35:35,492] Trial 9 finished with value: 0.8013770107656248 and parameters: {'kernel': 'linear', 'degree': 5}. Best is trial 7 with value: 0.7460811180026582.\n",
            "[I 2024-01-11 02:35:35,499] A new study created in memory with name: no-name-e2b9262c-8562-4ac1-a3b3-aec4b48945ff\n",
            "[I 2024-01-11 02:35:36,605] Trial 0 finished with value: 0.9670279674926923 and parameters: {'n_neighbors': 2, 'weights': 'distance'}. Best is trial 0 with value: 0.9670279674926923.\n",
            "[I 2024-01-11 02:35:37,252] Trial 1 finished with value: 0.920055707268437 and parameters: {'n_neighbors': 5, 'weights': 'uniform'}. Best is trial 1 with value: 0.920055707268437.\n",
            "[I 2024-01-11 02:35:37,744] Trial 2 finished with value: 0.9105769291102203 and parameters: {'n_neighbors': 8, 'weights': 'uniform'}. Best is trial 2 with value: 0.9105769291102203.\n",
            "[I 2024-01-11 02:35:38,249] Trial 3 finished with value: 0.9366330294534566 and parameters: {'n_neighbors': 3, 'weights': 'uniform'}. Best is trial 2 with value: 0.9105769291102203.\n",
            "[I 2024-01-11 02:35:38,739] Trial 4 finished with value: 0.9105769291102203 and parameters: {'n_neighbors': 8, 'weights': 'uniform'}. Best is trial 2 with value: 0.9105769291102203.\n",
            "[I 2024-01-11 02:35:39,258] Trial 5 finished with value: 0.9068748012848205 and parameters: {'n_neighbors': 9, 'weights': 'uniform'}. Best is trial 5 with value: 0.9068748012848205.\n",
            "[I 2024-01-11 02:35:39,753] Trial 6 finished with value: 0.935995063022442 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 5 with value: 0.9068748012848205.\n",
            "[I 2024-01-11 02:35:40,271] Trial 7 finished with value: 1.044028292874314 and parameters: {'n_neighbors': 1, 'weights': 'distance'}. Best is trial 5 with value: 0.9068748012848205.\n",
            "[I 2024-01-11 02:35:40,774] Trial 8 finished with value: 0.935995063022442 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 5 with value: 0.9068748012848205.\n",
            "[I 2024-01-11 02:35:41,281] Trial 9 finished with value: 0.935995063022442 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 5 with value: 0.9068748012848205.\n",
            "[I 2024-01-11 02:35:41,288] A new study created in memory with name: no-name-fbaca939-f7ea-4538-83e1-f7316ce52ea4\n",
            "[I 2024-01-11 02:36:39,970] Trial 0 finished with value: 0.8607601974441279 and parameters: {'n_estimators': 137, 'criterion': 'squared_error', 'max_depth': 8, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.8607601974441279.\n",
            "[I 2024-01-11 03:05:47,812] Trial 1 finished with value: 0.8224207471948041 and parameters: {'n_estimators': 200, 'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.8224207471948041.\n",
            "[I 2024-01-11 03:07:18,240] Trial 2 finished with value: 0.9085358615769129 and parameters: {'n_estimators': 22, 'criterion': 'absolute_error', 'max_depth': 2, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.8224207471948041.\n",
            "[I 2024-01-11 03:08:03,008] Trial 3 finished with value: 0.8532602321452212 and parameters: {'n_estimators': 95, 'criterion': 'squared_error', 'max_depth': 9, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.8224207471948041.\n",
            "[I 2024-01-11 03:10:07,710] Trial 4 finished with value: 0.8414483287173677 and parameters: {'n_estimators': 12, 'criterion': 'absolute_error', 'max_depth': 8, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.8224207471948041.\n",
            "[I 2024-01-11 03:26:03,860] Trial 5 finished with value: 0.8483266809785948 and parameters: {'n_estimators': 160, 'criterion': 'absolute_error', 'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.8224207471948041.\n",
            "[I 2024-01-11 03:26:18,963] Trial 6 finished with value: 0.8493529051219697 and parameters: {'n_estimators': 37, 'criterion': 'squared_error', 'max_depth': 9, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.8224207471948041.\n",
            "[I 2024-01-11 03:26:52,099] Trial 7 finished with value: 0.8619343528769605 and parameters: {'n_estimators': 82, 'criterion': 'friedman_mse', 'max_depth': 7, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.8224207471948041.\n",
            "[I 2024-01-11 03:27:19,015] Trial 8 finished with value: 0.9113871474897646 and parameters: {'n_estimators': 189, 'criterion': 'friedman_mse', 'max_depth': 2, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.8224207471948041.\n",
            "[I 2024-01-11 03:37:50,332] Trial 9 finished with value: 0.8197319678509033 and parameters: {'n_estimators': 90, 'criterion': 'absolute_error', 'max_depth': 8, 'min_samples_leaf': 10}. Best is trial 9 with value: 0.8197319678509033.\n",
            "[I 2024-01-11 03:37:50,335] A new study created in memory with name: no-name-1902bdf0-45b7-4dee-bf21-ca4af447213b\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:38:24,095] Trial 0 finished with value: 0.9503323034494369 and parameters: {'n_estimators': 80, 'learning_rate': 0.479439858945208, 'loss': 'linear'}. Best is trial 0 with value: 0.9503323034494369.\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:39:05,926] Trial 1 finished with value: 0.9675991638942161 and parameters: {'n_estimators': 116, 'learning_rate': 0.42314472363725575, 'loss': 'linear'}. Best is trial 0 with value: 0.9503323034494369.\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:39:43,747] Trial 2 finished with value: 1.0026823468904738 and parameters: {'n_estimators': 89, 'learning_rate': 0.8717759680597099, 'loss': 'exponential'}. Best is trial 0 with value: 0.9503323034494369.\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:40:03,407] Trial 3 finished with value: 0.9877700439265852 and parameters: {'n_estimators': 44, 'learning_rate': 0.3160620804670542, 'loss': 'square'}. Best is trial 0 with value: 0.9503323034494369.\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:41:27,295] Trial 4 finished with value: 0.9205115476563748 and parameters: {'n_estimators': 183, 'learning_rate': 0.07737609589436417, 'loss': 'linear'}. Best is trial 4 with value: 0.9205115476563748.\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:41:44,582] Trial 5 finished with value: 0.895908769677564 and parameters: {'n_estimators': 40, 'learning_rate': 0.23243125168634904, 'loss': 'exponential'}. Best is trial 5 with value: 0.895908769677564.\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:42:37,200] Trial 6 finished with value: 0.9498563656420845 and parameters: {'n_estimators': 141, 'learning_rate': 0.2622295630333951, 'loss': 'linear'}. Best is trial 5 with value: 0.895908769677564.\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:43:26,380] Trial 7 finished with value: 0.9526667211332733 and parameters: {'n_estimators': 110, 'learning_rate': 0.3347241835751715, 'loss': 'exponential'}. Best is trial 5 with value: 0.895908769677564.\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:44:48,403] Trial 8 finished with value: 1.0141140445192425 and parameters: {'n_estimators': 191, 'learning_rate': 0.672437345619686, 'loss': 'exponential'}. Best is trial 5 with value: 0.895908769677564.\n",
            "<ipython-input-11-3676543eb2d9>:83: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:45:24,302] Trial 9 finished with value: 0.9531954419685341 and parameters: {'n_estimators': 93, 'learning_rate': 0.48797724945283844, 'loss': 'linear'}. Best is trial 5 with value: 0.895908769677564.\n",
            "[I 2024-01-11 03:45:24,304] A new study created in memory with name: no-name-6b18f5fe-a834-42eb-a3d5-fe970d27f85d\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:46:04,437] Trial 0 finished with value: 0.861660251923801 and parameters: {'n_estimators': 106, 'learning_rate': 0.018830162846310505, 'loss': 'squared_error'}. Best is trial 0 with value: 0.861660251923801.\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:47:16,007] Trial 1 finished with value: 0.8240459983513955 and parameters: {'n_estimators': 183, 'learning_rate': 0.016362510417171537, 'loss': 'huber'}. Best is trial 1 with value: 0.8240459983513955.\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:48:38,770] Trial 2 finished with value: 0.7744555374718425 and parameters: {'n_estimators': 184, 'learning_rate': 0.28371658742864864, 'loss': 'huber'}. Best is trial 2 with value: 0.7744555374718425.\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:49:22,827] Trial 3 finished with value: 1.0532468422182797 and parameters: {'n_estimators': 102, 'learning_rate': 0.5163513796592771, 'loss': 'quantile'}. Best is trial 2 with value: 0.7744555374718425.\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:50:18,044] Trial 4 finished with value: 0.7857618935712298 and parameters: {'n_estimators': 136, 'learning_rate': 0.11113159169634182, 'loss': 'squared_error'}. Best is trial 2 with value: 0.7744555374718425.\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:50:26,380] Trial 5 finished with value: 0.8465135985169447 and parameters: {'n_estimators': 20, 'learning_rate': 0.14621763250912762, 'loss': 'squared_error'}. Best is trial 2 with value: 0.7744555374718425.\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:51:07,423] Trial 6 finished with value: 0.7860513257110313 and parameters: {'n_estimators': 91, 'learning_rate': 0.6057374596849076, 'loss': 'huber'}. Best is trial 2 with value: 0.7744555374718425.\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:52:21,032] Trial 7 finished with value: 0.7850660138400074 and parameters: {'n_estimators': 157, 'learning_rate': 0.2937557743059219, 'loss': 'absolute_error'}. Best is trial 2 with value: 0.7744555374718425.\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:53:34,680] Trial 8 finished with value: 0.7853112143332824 and parameters: {'n_estimators': 169, 'learning_rate': 0.11411017261766071, 'loss': 'absolute_error'}. Best is trial 2 with value: 0.7744555374718425.\n",
            "<ipython-input-11-3676543eb2d9>:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0)\n",
            "[I 2024-01-11 03:54:11,841] Trial 9 finished with value: 1.0917043083308915 and parameters: {'n_estimators': 80, 'learning_rate': 0.19604845151766623, 'loss': 'quantile'}. Best is trial 2 with value: 0.7744555374718425.\n",
            "[I 2024-01-11 03:54:11,844] A new study created in memory with name: no-name-deea3b37-6a6f-4b62-9a26-007ae0328b02\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 03:55:41,728] Trial 0 finished with value: 0.8159701334479962 and parameters: {'n_estimators': 177, 'max_samples': 0.4501653817579019, 'max_features': 0.6422390615889934}. Best is trial 0 with value: 0.8159701334479962.\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 03:56:02,864] Trial 1 finished with value: 0.8261681787105737 and parameters: {'n_estimators': 65, 'max_samples': 0.4632299147796711, 'max_features': 0.3897375430757769}. Best is trial 0 with value: 0.8159701334479962.\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 03:57:20,064] Trial 2 finished with value: 0.8133745382847544 and parameters: {'n_estimators': 150, 'max_samples': 0.4395035667114936, 'max_features': 0.665344682695957}. Best is trial 2 with value: 0.8133745382847544.\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 03:58:07,022] Trial 3 finished with value: 0.8170105545480676 and parameters: {'n_estimators': 129, 'max_samples': 0.5867828812817096, 'max_features': 0.4000851848323753}. Best is trial 2 with value: 0.8133745382847544.\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 03:58:43,700] Trial 4 finished with value: 0.8177167418241463 and parameters: {'n_estimators': 67, 'max_samples': 0.8404526311892928, 'max_features': 0.504111312001331}. Best is trial 2 with value: 0.8133745382847544.\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 03:59:45,406] Trial 5 finished with value: 0.829439047045634 and parameters: {'n_estimators': 186, 'max_samples': 0.13051216458581724, 'max_features': 0.7537306526453044}. Best is trial 2 with value: 0.8133745382847544.\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 04:01:38,090] Trial 6 finished with value: 0.8136690827719809 and parameters: {'n_estimators': 135, 'max_samples': 0.9137339462432458, 'max_features': 0.7057194620873796}. Best is trial 2 with value: 0.8133745382847544.\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 04:03:36,041] Trial 7 finished with value: 0.813829903168166 and parameters: {'n_estimators': 186, 'max_samples': 0.5308906119152886, 'max_features': 0.7334491109295173}. Best is trial 2 with value: 0.8133745382847544.\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 04:03:41,980] Trial 8 finished with value: 0.8420211477761319 and parameters: {'n_estimators': 11, 'max_samples': 0.599338711742659, 'max_features': 0.5347440836430966}. Best is trial 2 with value: 0.8133745382847544.\n",
            "<ipython-input-11-3676543eb2d9>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_samples = trial.suggest_uniform(\"max_samples\", 0.1, 1.0)\n",
            "<ipython-input-11-3676543eb2d9>:100: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  max_features = trial.suggest_uniform(\"max_features\", 0.1, 1.0)\n",
            "[I 2024-01-11 04:05:04,175] Trial 9 finished with value: 0.8131034002051796 and parameters: {'n_estimators': 148, 'max_samples': 0.5746732470926198, 'max_features': 0.6196518600295632}. Best is trial 9 with value: 0.8131034002051796.\n",
            "[I 2024-01-11 04:05:04,177] A new study created in memory with name: no-name-62d0ab5c-e6f2-4aa2-ae7d-84b58571ad74\n",
            "[I 2024-01-11 04:25:08,983] Trial 0 finished with value: 0.9133436001475147 and parameters: {'criterion': 'absolute_error', 'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.9133436001475147.\n",
            "[I 2024-01-11 04:26:06,414] Trial 1 finished with value: 0.8920241720458127 and parameters: {'criterion': 'friedman_mse', 'max_depth': 6, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.8920241720458127.\n",
            "[I 2024-01-11 04:26:57,772] Trial 2 finished with value: 0.8996540020489976 and parameters: {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.8920241720458127.\n",
            "[I 2024-01-11 04:28:07,125] Trial 3 finished with value: 0.9270122338925251 and parameters: {'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.8920241720458127.\n",
            "[I 2024-01-11 04:55:45,419] Trial 4 finished with value: 0.9038330241730457 and parameters: {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.8920241720458127.\n",
            "[I 2024-01-11 05:10:50,627] Trial 5 finished with value: 0.9398579988922364 and parameters: {'criterion': 'absolute_error', 'max_depth': 2, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.8920241720458127.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def evaluate_regression(model, X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    reg = copy.deepcopy(model)\n",
        "    reg.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
        "    y_pred = reg.predict(X_test)\n",
        "    rmse = mean_squared_error(y_pred, y_test)**0.5\n",
        "    r2 = r2_score(y_pred, y_test)\n",
        "    return rmse, r2\n",
        "\n",
        "# Define models with optimized hyperparameters\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(fit_intercept=False, positive=True),\n",
        "    \"Decision Tree Regressor\": DecisionTreeRegressor(criterion='friedman_mse', max_depth=6, min_samples_leaf=8),\n",
        "    \"SVR\": SVR(kernel='rbf', degree=4),\n",
        "    \"K Neighbors Regressor\": KNeighborsRegressor(n_neighbors=6, weights='distance'),\n",
        "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=115, criterion='friedman_mse', max_depth=9, min_samples_leaf=6),\n",
        "    \"AdaBoost Regressor\": AdaBoostRegressor(n_estimators=101, learning_rate=0.3657, loss='exponential'),\n",
        "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(n_estimators=186, learning_rate=0.2717, loss='squared_error'),\n",
        "    \"Bagging Regressor\": BaggingRegressor(n_estimators=165, max_samples=0.9579, max_features=0.8558),\n",
        "    \"Extra Trees Regressor\": ExtraTreesRegressor(criterion='absolute_error', max_depth=6, min_samples_leaf=8),\n",
        "    \"MLP Regressor\": MLPRegressor(\n",
        "        hidden_layer_sizes=(120,38,102),\n",
        "        #activation='tanh',\n",
        "        #solver='lbfgs',\n",
        "        alpha=8e-06,\n",
        "        #learning_rate='invscaling',\n",
        "        learning_rate_init=0.0007\n",
        "    )\n",
        "}\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    rmse, r2 = evaluate_regression(model, X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "    print(f\"{model_name}: RMSE value for test set: {rmse}, R^2 value: {r2}\")\n",
        "\n",
        "    # Save the model\n",
        "    joblib.dump(model, f\"{model_name}_model.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHq7g9h4DO-U",
        "outputId": "94a2b88a-4c82-47d8-943f-1c3c40f9164d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression: RMSE value for test set: 0.8062769641788758, R^2 value: 0.12332637739793917\n",
            "Decision Tree Regressor: RMSE value for test set: 0.7570061552533178, R^2 value: -0.625027096396116\n",
            "SVR: RMSE value for test set: 0.6238500514747718, R^2 value: -0.39130691978513354\n",
            "K Neighbors Regressor: RMSE value for test set: 0.7796856206749837, R^2 value: -1.5963027906578544\n",
            "Random Forest Regressor: RMSE value for test set: 0.6619357899963417, R^2 value: -0.6302610130631294\n",
            "AdaBoost Regressor: RMSE value for test set: 0.7179579637293303, R^2 value: -2.4817610347160994\n",
            "Gradient Boosting Regressor: RMSE value for test set: 0.5964413269058734, R^2 value: 0.33006371030944015\n",
            "Bagging Regressor: RMSE value for test set: 0.6140184138038319, R^2 value: -0.20957966014894125\n",
            "Extra Trees Regressor: RMSE value for test set: 0.7500779767671344, R^2 value: -0.7385898857660012\n",
            "MLP Regressor: RMSE value for test set: 0.6183894886210476, R^2 value: 0.41999649489387814\n"
          ]
        }
      ]
    }
  ]
}